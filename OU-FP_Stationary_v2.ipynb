{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "# Define model architecture\n",
    "class DCGMNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the model.\"\"\"\n",
    "\n",
    "    def __init__(self, X_low, X_high,\n",
    "                 input_dim, output_dim,\n",
    "                 n_layers_FFNN, layer_width,\n",
    "                 activation_FFNN,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.X_low = X_low\n",
    "        self.X_high = X_high\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.n_layers_FFNN = n_layers_FFNN\n",
    "        self.layer_width = layer_width\n",
    "        \n",
    "        self.activation_FFNN = activation_FFNN\n",
    "        # print(activation_FFNN)\n",
    "        \n",
    "        # Define NN architecture\n",
    "        # self.initial_scale = tf.keras.layers.Lambda(\n",
    "        #     lambda x: 2.0*(x - X_low)/(X_high - X_low) - 1.0)\n",
    "        self.initial_scale = tf.keras.layers.Dense(layer_width)\n",
    "        \n",
    "        self.hidden = [tf.keras.layers.Dense(layer_width,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation_FFNN),\n",
    "                                             kernel_initializer=kernel_initializer)\n",
    "                       for _ in range(self.n_layers_FFNN)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        Z = self.initial_scale(X)\n",
    "        for i in range(self.n_layers_FFNN):\n",
    "            Z = self.hidden[i](Z) + Z\n",
    "        return self.out(Z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aiyagari problem parameters\n",
    "kappa = 0.5  # mean reversion rate\n",
    "theta = 0.0  # mean reversion level\n",
    "sigma = 2    # volatility\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "nSim_x_interior = 1000\n",
    "\n",
    "X_low = np.array([-4])  # wealth lower bound\n",
    "X_high = np.array([4])          # wealth upper bound\n",
    "\n",
    "\n",
    "# neural network parameters\n",
    "num_layers_FFNN = 6\n",
    "num_layers_RNN = 0\n",
    "nodes_per_layer = 50\n",
    "starting_learning_rate = 0.001\n",
    "shrinkstep = 20000\n",
    "shrinkcoef = 0.95\n",
    "activation_FFNN = 'tanh'\n",
    "# Training parameters\n",
    "sampling_stages  = 2000   # number of times to resample new time-space domain points\n",
    "steps_per_sample = 10    # number of SGD steps to take before re-sampling\n",
    "\n",
    "\n",
    "dim_input = 1\n",
    "dim_output = 1\n",
    " \n",
    "model = DCGMNet(X_low, X_high,  \n",
    "                 dim_input, dim_output, \n",
    "                 num_layers_FFNN, nodes_per_layer,\n",
    "                 activation_FFNN)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(theta, kappa, sigma, nSim):\n",
    "    ''' Simulate end point of Ornstein-Uhlenbeck process with normally \n",
    "        distributed random starting value.\n",
    "    \n",
    "    Args:\n",
    "        alpha: mean of random starting value\n",
    "        beta:  standard deviation of random starting value\n",
    "        theta: mean reversion level\n",
    "        kappa: mean reversion rate\n",
    "        sigma: volatility \n",
    "        nSim:  number of simulations\n",
    "        T:     terminal time        \n",
    "    '''  \n",
    "        \n",
    "    # simulate initial point based on normal distribution\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta \n",
    "    v = np.sqrt(sigma**2 / ( 2 * kappa) )\n",
    "    \n",
    "    # simulate endpoint\n",
    "    X = np.random.normal(m,v,size=(nSim,1))    \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def sampler(nSim_x_interior):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_x_interior: number of space points in the interior of the function's domain to sample \n",
    "    ''' \n",
    "    \n",
    "    # Sampler #1: domain interior\n",
    "    x_interior = np.random.uniform(low=X_low[0], high=X_high[0], size=[nSim_x_interior, 1])\n",
    "    \n",
    "    return x_interior\n",
    "\n",
    "\n",
    "def compute_loss(model, x_interior):\n",
    "    ''' Compute total loss for training.\n",
    "        NOTE: the loss is based on the PDE satisfied by the negative-exponential\n",
    "              of the density and NOT the density itself, i.e. the u(t,x) in \n",
    "              p(t,x) = exp(-u(t,x)) / c(t)\n",
    "              where p is the density and c is the normalization constant\n",
    "    \n",
    "    Args:\n",
    "        model:      DGM model object\n",
    "        t:          sampled (interior) time points\n",
    "        x_interior: sampled space points in the interior of the function's domain\n",
    "        x_initial:  sampled space points at initial time\n",
    "        nSim_t:     number of (interior) time points sampled (size of t)\n",
    "        alpha:      mean of normal distribution for process starting value\n",
    "        beta:       standard deviation of normal distribution for process starting value\n",
    "    ''' \n",
    "    \n",
    "    # Loss term #1: PDE\n",
    "    \n",
    "\n",
    "    # for each simulated interior time point\n",
    "        \n",
    "    # make vector of current time point to align with simulated interior space points   \n",
    "    x_interior = tf.cast(tf.reshape(x_interior, shape=[\n",
    "                            nSim_x_interior, 1]), tf.float32)\n",
    "    # print(t_vector)\n",
    "    # compute function value and derivatives at current sampled points\n",
    "    u    = model(tf.stack([x_interior[:,0]], axis=1))\n",
    "    u_x = tf.gradients(u, x_interior)[0]\n",
    "    u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "    # psi function: normalized and exponentiated neural network\n",
    "    # note: sums are used to approximate integrals (importance sampling)\n",
    "    # psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "    # print(u_t)\n",
    "\n",
    "    # PDE differential operator\n",
    "    # NOTE: EQUATION IN DOCUMENT IS INCORRECT - EQUATION HERE IS CORRECT\n",
    "    diff_f = kappa * u + kappa*(x_interior - theta)*u_x + 0.5*sigma**2* u_xx\n",
    "    \n",
    "    # compute L2-norm of differential operator and attach to vector of losses\n",
    "    currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "\n",
    "    # average losses across sample time points \n",
    "    L1 = currLoss\n",
    "    \n",
    "    # Loss term #2: boundary condition\n",
    "        # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term #3: initial condition\n",
    "    \n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0\n",
    "    # i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    \n",
    "    \n",
    "    # init_t = tf.cast(0, tf.float32)\n",
    "    # x_interior = tf.cast(tf.reshape(x_interior, shape=[\n",
    "    #     nSim_x_interior, 1]), tf.float32)\n",
    "    # t_vector = init_t * tf.ones_like(x_interior)\n",
    "    \n",
    "    # fitted_pdf = model(\n",
    "    #     tf.stack([0*tf.ones_like(x_initial)[:,0], x_initial[:,0]], axis=1))\n",
    "    # # target pdf - normally distributed starting value\n",
    "    # # NOTE: we are only comparing the exponential terms \n",
    "    # target_pdf = tf.cast(0.5*(x_initial - alpha)**2 / (beta**2),tf.float32)\n",
    "    \n",
    "    # # average L2 error for initial distribution\n",
    "    # L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "\n",
    "    return L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_grad(model, x_interior):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss1 = compute_loss(model, x_interior)\n",
    "        loss = loss1\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    del tape\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0606391542\n",
      "100 7.31016087e-07\n",
      "200 1.12261102e-06\n",
      "300 1.17734501e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\33678\\Ineq_S\\OU-FP_Stationary.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33678/Ineq_S/OU-FP_Stationary.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m x_interior \u001b[39m=\u001b[39m sampler(nSim_x_interior)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33678/Ineq_S/OU-FP_Stationary.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps_per_sample):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/33678/Ineq_S/OU-FP_Stationary.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_step(model, x_interior)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33678/Ineq_S/OU-FP_Stationary.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m hist\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33678/Ineq_S/OU-FP_Stationary.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=starting_learning_rate)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=starting_learning_rate)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x_interior):\n",
    "    # Compute current loss and gradient w.r.t. parameters\n",
    "    loss, grad_theta = get_grad(model, x_interior)\n",
    "\n",
    "    # Perform gradient descent step\n",
    "    optimizer.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "hist = []\n",
    "\n",
    "for i in range(sampling_stages):\n",
    "\n",
    "    # sample uniformly from the required regions\n",
    "\n",
    "    x_interior = sampler(nSim_x_interior)\n",
    "\n",
    "    for _ in range(steps_per_sample):\n",
    "        loss = train_step(model, x_interior)\n",
    "    \n",
    "    hist.append(loss.numpy())\n",
    "    \n",
    "    if i%100==0:\n",
    "        tf.print(i,loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]),\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEZCAYAAADCJLEQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlsElEQVR4nO3deZhcVZ3/8feHkAXIhqFBSWiSCDwgyGb/CKgsKmIUBZFBYBJGYSAQZVhESSIZF0RMiOKCoITFhUSJCDIywIRdoiDSCYsgsiQmGcIACdBZzMKS7++PezsURXV3VXd131vVn9fz3Ke6zzn31vcm3fXte+655ygiMDMzy5vNsg7AzMysFCcoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLpZpPUJL6SJohabmk1ZKul7RNGftNlBSSphaVh6S1ktYUbEO67wzMzKyUmk9QwGTgSGAMMCItu6a9HSTtCJwD/LWNJodFxMCCbWXVojUzs7LUQ4KaAEyPiEVpIjkXGCtpZDv7XAWcB7zcA/GZmVknbJ51AF2Rdr01AvNbyyJioaRVwJ7A4hL7nAqsjYg5kia2cejrJPUFFpIkvxvaeP8JJAmSrbba6n277rprV07HzKzXmT9//oqIaChVV9MJChicvhZ3wbUU1G0iqRGYCuzfzjEPBf6Ufn0kMFvSURHxP8UNI2ImMBOgqakpmpubKwrezKy3k7Skrbpa7+Jbnb4WD2IYCqwq0f5K4IKIWNbWASPizohYn25zgFnAuGoEa2Zm5avpBBURLcBSYN/WMkmjSa6eHi2xy0eBCyWtkLQC+AAwRdK8dt5mI6CqBW1mZmWp9S4+SLrYJkm6G3gJmA7MjYjFJdruUPT9dcA84HsAkvYAtgQeBgI4HDgBOK47Ajczs7bVQ4KaBmwNPAj0B24HxgNIGgdcHhEDASLi2cIdJW0AVkXEC2lRA/BjYCTwKskgiZMi4vfdfxpmZlZIXrCwOjxIwsyscpLmR0RTqbqavgdlZmb1ywnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxyqeYTlKQ+kmZIWi5ptaTrJW1Txn4TJYWkqUXlO0m6Q9I/JT0r6Zzui97MzNpS8wkKmAwcCYwBRqRl17S3g6QdgXOAvxaV9wFuAp4gWbzwCJLVeo+tcsxmZtaBekhQE4DpEbEoIlYC5wJjJY1sZ5+rgPOAl4vKDwJ2BKZExNqIWABcDpxW/bDNzKw9NZ2gJA0BGoH5rWURsRBYBezZxj6nAmsjYk6J6r2ApyJiTUHZgrTczMx60OZZB9BFg9PXlUXlLQV1m0hqBKYC+7dxvEHlHis93gSSKzgaGxvLidfMzMpU01dQwOr0dUhR+VCSq6hiVwIXRMSydo5X7rGIiJkR0RQRTQ0NDWUFbGZm5anpBBURLcBSYN/WMkmjSa54Hi2xy0eBCyWtkLQC+AAwRdK8tP4RYBdJWxXss09abmZmPaimE1RqJslIu1GSBgPTgbkRsbhE2x1I7iftnW7NwKXAv6T19wJLSJLYFpL2Bk4lGShhZmY9qB4S1DSSoeEPAsuAPsB4AEnjJG0a8BARzxZuwAZgVUS8kNa/AXwK2AN4CbgFmBER1/bkCZmZGSgiso6hLjQ1NUVzc3PWYZiZ1RRJ8yOiqVRdPVxBmZlZHXKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXKr5BCWpj6QZkpZLWi3peknbtNH2QEkLJL0saWX69WeK2oSktZLWFGxDeuZszMysVc0nKGAycCQwBhiRll3TRtsngaOAYcBQ4CxglqTditodFhEDC7aVVY/azMzatXnWAVTBBOD8iFgEIOlc4BlJIyNicWHDiHix9WtJmwEbSZL0TsATPRaxmZl1qKavoNKut0ZgfmtZRCwEVgF7trNfC7ABmAc8ANxW1OQ6SSskPVDcBVh0nAmSmiU1L1++vPMnYmZmb1PTCQoYnL4Wd8G1FNS9TUQMBQaSdPfdArxeUH0oMIqku/BiYLaksW0cZ2ZENEVEU0NDQ2fiNzOzNtR6glqdvhYPYhhKchXVpojYEBE3AgcDJxeU3xkR69NtDjALGFe1iM3MrCw1naAiogVYCuzbWiZpNMnV06NlHmZzYOd26jcC6mSIZmbWSTWdoFIzgUmSRkkaDEwH5hYPkACQdLSk90raXNIASacAHwbmpvV7SNpPUj9JfSV9GjgB+E2PnY2ZmQH1MYpvGrA18CDQH7gdGA8gaRxweUQMTNu+K23/LuBVkmHnx0fE7Wl9A/BjYGRavxA4KSJ+3yNnYmZmmygiso6hLjQ1NUVzc3PWYZiZ1RRJ8yOiqVRdPXTxmZlZHXKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXHKCMjOzXKr5BCWpj6QZkpZLWi3peknbtNH2QEkLJL0saWX69WeK2uwk6Q5J/5T0rKRzeuZMzMysUM0nKGAycCQwBhiRll3TRtsngaOAYcBQ4CxglqTdIEl2wE3AEySLFx5Bslrvsd0Uu5mZtaEeEtQEYHpELIqIlcC5wFhJI4sbRsSLEbEkklUaBWwk+TfYKW1yELAjMCUi1kbEAuBy4LQeOA8zMytQ0wlK0hCgEZjfWhYRC4FVwJ7t7NcCbADmAQ8At6VVewFPRcSaguYL0vJSx5kgqVlS8/Lly7twJmZmVqymExQwOH1dWVTeUlD3NhExFBhI0t13C/B6WjWokmNFxMyIaIqIpoaGhkriNjOzDtR6glqdvg4pKh9KchXVpojYEBE3AgcDJxccr+JjmZlZ9dV0goqIFmApsG9rmaTRJFc8j5Z5mM2BndOvHwF2kbRVQf0+abmZmfWgmk5QqZkkI+1GSRoMTAfmRsTi4oaSjpb0XkmbSxog6RTgw8DctMm9wBLgQklbSNobOJVkoISZmfWgekhQ00iGhj8ILAP6AOMBJI2TVDjg4V3ADST3lZ4DTgKOj4jbASLiDeBTwB7ASyT3p2ZExLU9ciZmZraJkhHX1lVNTU3R3NycdRhmZjVF0vyIaCpVVw9XUGZmVoecoMzMLJecoMzMLJecoMzMLJecoMzMLJecoMzMLJecoMzMLJecoMzMLJecoMzMLJecoMzMLJc2r3QHSe8F9gPeCQwAXgaeAu6LiFeqG56ZmfVWZSWodAmLicA4YDuSpdJbSFalHQpsCWyU9AfgSmBORGzshnjNzKyX6LCLT9KVwOPA3sD5JOsjDYiIhogYEREDgW1JZgH/K3AR8ISkD3Zb1GZmVvfKuYJaD+waEUvaahARK4BbgVslfQk4BhhenRDNzKw36vAKKiJOby85lWi/MSLmRMScroVWHkl9JM2QtFzSaknXS9qmjbafkHSXpBWSXpE0T9KBRW1C0lpJawq24mXgzcysm1U0ik/SDySpu4LppMnAkcAYYERadk0bbbcGLgF2AhqAX5Fc9e1Q1O6wiBhYsK3shrjNzKwdlQ4zPx64UdKWpSolfbzrIVVsAjA9IhalieRcYKykkcUNI2J2RPwuIloi4vWI+AmwDii5WJaZmWWn0gS1P8nVxzxJ72otlPQxSQ8A/13N4DqSdr01AvNbyyJiIbAK2LOM/fcEhgGPFVVdl3YDPiDpM+3sP0FSs6Tm5cuXd+oczMystIoSVET8A3g/sAL4i6STJN1HMkBiJXBI1SNs3+D0tbgLrqWgriRJ2wK/BS6KiKcLqg4FRpF0F14MzJY0ttQxImJmRDRFRFNDQ0Mnwjczs7ZUPJNE2o32XZL7OVeQPKx7QEQcFhHzqhxfR1anr8WDGIaSXEWVJGl74G7gNmBKYV1E3BkR69NtDjCL5PkvMzPrQZUOkviYpD8C/wPcB1wOvIek26/HRUQLsBTYtyDG0SRXT4+W2ie9NzUPuDUdoRgdvM1GIG8DQ8zM6l6lUx3dSnLlcXBE/BFA0sPA1ZJ2iYivVzm+cswEJkm6G3gJmA7MjYjFxQ0l7QrcAfw8IqaWqN+DZFaMh4EADgdOAI7rruDNzKy0Srv4DomIj7QmJ0juwwCfBM6QdG1VoyvPNOAm4EFgGdAHGA8gaZykNQVtJ5E8QHxW0XNOrV14DcDPgFeAF4GpwEkR8fueORUzM2uljnu4yjyQtDtwU0SMrsoBa0xTU1M0NzdnHYaZWU2RND8iSj7qU7XlNiLicZKHZc3MzLqsqutBRYQfBjIzs6rwgoVmZpZLVUtQkg5KR8mZmZl1WTWvoO4BHpd0p6TDq3hcMzPrhaqZoD5EMtx8HnBGFY9rZma9UEUP6ko6BLgvIl4trouIP6Rf3tr1sMzMrLer9ArqTpKl380sR6S2N7Na1WGCklR4ldXmj7ukMZKeq0pUZmbW65VzBXWepOcl3UE6P52k/UosWtiPZG0lMzOzLivnHtSvSOam25PkCuorwH8CGyUtAh4B/g4cBDzVTXGamVkv02GCShfzexpA0hHAp4DnSRJW63YEyfpLp3VbpGZm1qtUNIovIrYt+HYJySziZmZmVVfpelBmVmM6GslXpQUNzKqunFF8J0jqU8lBJe0k6cDOh1XRe/WRNEPSckmrJV0vaZs22n5C0l2SVkh6RdK84jjT2O+Q9E9Jz0o6pyfOw8zM3qqcUXznAAslfUvSXm01kjQsXSDwJuAh4F3VCrIDk4EjSZb6GJGWXdNG262BS0iWqG8gGQByq6QdIEl2JN2WT6T1R5Cs1ntst0VvZmYllbVgYfoB/R/A+4E1JB/gK4ANwFBgFNBIMtpvFjAjIpZ1T8hvi20JcH5EXJV+/27gGWBUqWXfS+y/HJgQEb+T9CHgZmDbiFiT1n8L+GBEfKi943jBQstSVx7IdRefZam9BQvLugcVEXOAOemH/6HAvsA7ga2AF4B7gT8B90TEa1WJugyShpAkxvkFsS6UtIpkdOHiDvbfk+TZrcfSor2Ap1qTU2oB8MU29p8ATABobGzs3EmYmVlJlY7iWwgs7KZYOmNw+rqyqLyloK4kSdsCvwUuSofSAwyq5FgRMROYCckVVLlBm5lZxyqdLLYvydXEWOAdwHPAHcAvI2JV9cPr0Or0dUhR+VCS57JKkrQ9cDtwGzCl6HgVHcvMzLpHpZPFXgJcDPQh6VbrD3wLWCzp6CrH1qGIaAGWknQ5AiBpNMkVz6Ol9pE0kmRJkFsj4vR46024R4BdJG1VULZPWm6WmfYmg/WEsFavKk1QxwBfi4iPRsTEiPg4sAPwPWC2pE9WPcKOzSQZaTdK0mBgOjC31ACJdMXfPwK/jogvlzjWvSQPIF8oaQtJewOnApd3V/BmZlZapQkqSD7g3yyIWBMR3wa+D1xQrcAqMI1kaPiDwDKSq7vxAOmw98IBD5OA4cBZktYUbOMAIuINkqmc9gBeAm4hGZF4bY+djdUsX+WYVVdZw8w3NZZmAS9FxJkl6g4h6TbbomrR1RAPM7funLEhywTnYejWndobZl7pFdRC4ERJ35a0dVHdgcDjnQnQzMysWKVz8f0HMJBk5NsZkppJnjXaEXg3cHhVozMzs16r0ueg3iGpkeSB1tbtAyTJScA8SX8lGfX2SERcWeV4zayHtde96O4/604Vz2YeEUtJhnZvWmojXV13T95MWvsC/wY4QZmZWadUZbmNiFgL/DndzMzMuqzSQRJmZmY9wgsWmlXAzzOZ9RwnKLMe4pVtzSrjLj4zM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8ulmh9mLqkPyZpQnwcGkCzjfmpErCjRdjhwGbA30AicEBGzitoEsA7YWFA8PCJWdkf8ZlYFr7wCTz4Jzz8PL7wAq1bBxo3J1r8/vOMdyTZiBOy8MwwalHXEVoaaT1DAZOBIYAzJIoNXA9cAHy/RdiNJArsIaG8RwsMi4o/t1JtZVjZuhEcegTvvhHvugYcfhmXLKjvGdtvBe98L++//5jZsWHdEa11QDwlqAnB+RCwCkHQu8IykkcXLvkfE/wGXpu3e6OlAzayTIuChh2D2bLj2Wnjuua4d74UXku2OO5LvJWhqgk98ItmammAz3wHJWk0nKElDSLrq5reWRcRCSatIZldf3MlDXyepL8kCjdMj4oY23n8CSYKksbGxk29lZm167TX47W/h4ouh1IrVO+4IBxyQXA3tvjsMHw7vfCcMGQJ9+iRJZt26pAtwxQpYvBiefhqeegrmz4e//S1JfhHw4IPJ9s1vJl2Bxx0H//qvsPfenuMqIxUt+Z43knYgWfpjdET8o6B8CXBe8f2lon0XA1NL3IP6CPCn9NsjgZ8DR0XE/7QXi5d87x2683OqvV/FvH4+dtvHxxtvwC9/Cd/4Bixd+ta6gw+GY4+Fww6D0aO79o+zciX85S9w111wyy3w6KNvb7PbbnDyyfD5zyf3sayq2lvyvdYT1FDgFWCfiHi4oHwlyQCI37ez72JKJKgS7a4ABkTECe21c4LqHZygqqvkOd92G5xzDjz22JtlDQ0wcWKSKHbYofsCevZZuOkm+PWvYd68t9YNGJAkxtNOgzFj6vc/pYe1l6BqupM1IlpIrqD2bS2TNBoYDJT4U6hTNpKsFmxm3WnFChg/Hj72sTeT0/Dh8NOfwpIlSddbdyYnSLr2Jk6Ee+9N3nPaNNhll6Ru/Xr4xS+SLsUPfABuvDEZsGHdpqYTVGomMEnSKEmDgenA3OIBEq0kDZA0gCTp9E2/3zyt20PSfpL6Seor6dPACcBveuRMzHqrG25IutJmz06+HzQILrwwuVd06qmwxRY9H1NjI0yaBH//ezKY4uijk/taAPffD0cdBe95D1x1FWzY0PPx9QL1kKCmkSw//yCwDOgDjAeQNE7SmqL269KtkWRI+jpgalrXAPyMpNvwxbT8pPa6Cs2sC9avh9NPTz78V6SPLh5xBDzxBEyZAltumW18kHTlfeQjyWCNpUvhq1+FoUOTuiefTLodR4+GSy5JzseqpqbvQeWJ70H1Dr4HVT2NLGHJPkclw8ch+dD/6U/hs5/N/wmvXg1XXJGMLix8Bmv4cDjvPDjppOQBYetQ3d6DMrPaNIY/8xf2ezM5jRmTfH3ssflPTpB0QX7pS7BoEVx9dXIFBUmy+sIXktkqZs6EV1/NNs4a5wRlZj3qs8zhHg5hO15MCs44IxkxN3JkpnF1Sr9+cOKJyX2qq6568xz+93+Te2e77ZaMCPRgik5xgjKzHjORy5jDcQxgA6/TB37yE/jhD6Fv36xD65q+fZNuvSefTK6cWh/cX7Qoedi3qQnmzu3GB8fqkxOUmfWIr3ARl/FFAFYymI9zK5p4GhIlt5rUrx+cckoyW8Ull8C22yblDz0EY8fCoYcms1VYWZygzAq09WHZEx+aWb1vT/g63+AiJgGwgmF8iLu5g49mHFU36tcvGZ34zDPJ81sDBybld90F++0HxxyTDLSwdjlBmVm3msQ0vsE3Afg/3skh3MNDbz5bX98GDYKvfS3p6jvzzDe7MpcufTNpWZucoKzXqecrlbyZyGVMYwoAz/EuDuJeHmePjKPKQEMD/OAHyYPHJ5yQzFDhH7gO1fRs5maWX+OYteme0wqGcSh38Aw7ZxxVxkaOTCbBtbI4QVnd8R+m2TuU2/k5nwdgFYP4GHN5gvdkG5TVHCcoM6uq3fgbv+Vf2Jw3WE9/Psl/s4D3VXycjv7Q8Ijt+ud7UGZWNQ28yM0czhBWAfA5fsE8Dso4KqtVTlBmVhX9Wc+NfJpR6ULW53EBv+HYbIOymuYEZWZV8UPO5P3cD8DP+RwX8tWMI7Ja5wRlZl12IldzKjMBuI8DmMBMvM6ndVXNJyhJfSTNkLRc0mpJ10vapo22wyX9l6QlkkLS+BJtdpJ0h6R/SnpW0jndfxZmtWsfFnAZXwDgBbblGK7jNfplHJXVg5pPUMBk4EhgDDAiLbumjbYbgduAfwWeLa6U1Idk8cMnSBYvPIJktV53pJuVsDUvcz1Hb5r89Vjm8BzDsw7L6kQ9JKgJwPSIWBQRK4FzgbGSRhY3jIj/i4hLI+JPwBsljnUQsCMwJSLWRsQC4HLgtO4L36xWBVfx75sGRUxiOn/gkEwjsvpS0wlK0hCSpdvnt5ZFxEJgFbBnJw65F/BURBQuE78gLS/1/hMkNUtqXr58eSfezqx2TWAmR3EjADdwFBfzpWwDsrpT0wkKGJy+riwqbymoq8SgSo4VETMjoikimhoaGjrxdma1aTf+xvc5G4BnGc7JXIkHRVi11XqCap2vfkhR+VBInxSs/HjVOpZZXerPen7N8WzJOjYixjOLV3hH1mFZHarpBBURLcBSeHPufkmjSa54Hu3EIR8BdpG0VUHZPmm5mQHTmMxe6a/XhXzV952s29R0gkrNJBlpN0rSYGA6MDciFpdqLGmApAEk/RF90+9b5yS8F1gCXChpC0l7A6eSDJQw6/UO5h7O4ocA/JkxfJOvZxyR1bN6SFDTSIaGPwgsA/oA4wEkjZO0pqj9unRrBK5Ov54KEBFvAJ8C9gBeAm4BZkTEtd1/Gmb5thVruJqTAFjLFoxnFq/TN+OorJ7V/GzmaVL5croV180GZheVtXsnNyKeAT5SzRjN6sE0JjOafwAwmWksZKeMI7J6Vw9XUGbWzQ7hbk7nUgD+wEH8mNMzjsh6g5q/grL61N5aQF4HqGcVd+2dxNVEL/7b1utU9Zze+1NmZmWZxuRNs0VMZhqLeHe2AVmv4QRlZm0aw5/5ApcBcC8HumvPepQTlJmVtDmvcTmnshnBBvpxMlf26q4963n+aTOzks7kh295IPdpdsk4IuttPEjCzN6mkSWbHsJ9kl2YxuSMI3q7jgYrtMcDGWqDr6DMrEhwKV9kK9YCcCqX8yr9M47JeiNfQZnZW3yGG/gkNwPwcz5Xl3PtdeXqy3qOr6DMbJOBrOZHnAHACobxZb6bcUTWmzlBmdkmU7mA4TwHwFeYwUtsk3FE1ps5QZkZADvzFGfzfQDu4wB+wecyjsh6OycoMwOCH3AW/XiNjYgz+JGfebLMeZCE1Rzf4K6+w7mZT3ArAFfx78ynKeOIzOrgCkpSH0kzJC2XtFrS9ZLa7DiXNFbS45LWSXpM0mFF9SFpraQ1BVvxMvBmdaMfG/gBZwHQwhC+yoXZBmSWqvkEBUwGjgTGACPSsmtKNUyXg78B+A4wJH39naSRRU0Pi4iBBdvKboncLAfO5vvsxEIAvsb5rKAh44jMEvWQoCYA0yNiUZpIzgXGlkg6AJ8D5kfErIh4NV3QcEFabtbrbM8ypnIBAI+xOz9hYsYRmb2pphNU2vXWCMxvLYuIhcAqYM8Su+xV2Da1IC0vdJ2kFZIekPSZKoZslisXcS4D+ScAZ/AjL+FuuVLTCQoYnL4Wd8G1FNQVGlRG20OBUSTdhRcDsyWNLfXmkiZIapbUvHz58soiN8vY/tzPOH4FwHX8C3fz4YwjMnurWk9Qq9PX4kEMQ0muokq1b7dtRNwZEevTbQ4wCxhX6s0jYmZENEVEU0OD++2tlgTf4xwA1tPfM0ZYLtV0goqIFmApsG9rWToQYjCk6wS81SOFbVP7pOVt2Qh4YLPVlaO5nvdzPwA/4CyWsmPGEZm9XU0nqNRMYJKkUZIGA9OBuRGxuETbXwJNko6X1FfS8cD7gF8ASNpD0n6S+qX1nwZOAH7TI2di1gP68uqm5TNWMIzvMCXjiMxKq4cHdacBWwMPAv2B24HxAJLGAZdHxEBIBlCkgx6+B1wNLAKOKkhmDcCPgZHAq8BC4KSI+H1PnUxv4YdtszORn2waVv5Nvs6qt/V6m+WDwit3VUVTU1M0NzdnHUbNcILKxhBaWMi7GcbLPM1O7M7jvEa/rMOqK/5IrYyk+RFRcuqSeujiM7MyTeE7DONlACYx3cnJcs0JyqyXaGQJZ/JDAP7IB/gdR2UckVn76uEelOWUu/Hy5ducxwA2AKTDyv0fZPnmBGVt6ijBuK+9dryPZsYzG4DfcAwPsH/GEdWv9n5v/DtTGXfxmdW9YAZfAeBV+jKF72Qcj1l5nKDM6tzh3MyHuAeAS/kii3h3tgGZlckJyqyO9eH1TVdPLQzhAqZmHJFZ+ZygzOrYv3MVu/F3AC5gKi8zLOOIzMrnQRK9XFdG2nmUXr4NZDXn8zUAFrMjP+b0jCMyDzyqjK+gzOrUV5jBdrwIwFe5kA0MyDgis8r4CsqsDm3Psk1LaDxIE9dyXMYRWTl8hfVWTlBmdeh8vsaWrAOSh3LDnSV1oSvPWNXi81n+qTWrM+/lUU7kZwD8F0dwLwdnHJFZ5zhBmdWZiziXzQhepw+TmJ51OGad5gRlVkcO5XbGMheAKziFJ9k144jMOq/mE5SkPpJmSFouabWk6yVt0077sZIel7RO0mOSDiuq30nSHZL+KelZSed0/zl0bTMD2Iw3+C5fBmA1A/kG38g2IOtR9fg5UfMJCpgMHAmMAUakZdeUaihpNHAD8B1gSPr6O0kj0/o+wE3AEySr6x5Bspz8sd0Yv1lVnMA17MWjAExnEi+yXcYRWa3I6x/J9ZCgJgDTI2JRRKwEzgXGtiadIp8D5kfErIh4NSJmAwvScoCDgB2BKRGxNiIWAJcDp3X7WZh1wRas3TSN0TK252K+lHFEZl1X08PMJQ0BGoH5rWURsVDSKmBPYHHRLnsVtk0tSMtb65+KiDVF9V9s4/0nkCRIgDWSnqzwFLYBVlS4T4k4unqEqqjKueREzZ3LOmCHTd89B2zV+k3NnUs7fC75tI3UpXPZsa2Kmk5QwOD0dWVReUtBXaFBbbTdvYP6UsciImYCM8uKtARJzRHR1Nn988Tnkk8+l3zyuZSn1rv4VqevQ4rKhwKr2mjfXtuO6s3MrIfUdIKKiBZgKbBva1k6EGIwpHeL3+qRwrapfdLy1vpdJG3VRr2ZmfWQmk5QqZkkI+1GSRoMTAfmRsTiEm1/CTRJOl5SX0nHA+8DfpHW3wssAS6UtIWkvYFTSQZKdFfs9cLnkk8+l3zyuZRBkddJmMqUDg2fDnwe6A/cDkyIiBWSxgGXR8TAgvZjge8Bo4FFwNkRcVtB/U4kCekAkvtPF0fEd3vmbMzMrFXNJygzM6tP9dDFZ2ZmdcgJyszMcskJKgckHSfp0XT+v+clnZd1TF0haStJCyW9nnUsnSGpv6TLJT2dzu+4NJ3vsSaWpK10fsq8kjQ9nTdzlaTnJF0h6R1Zx9VVkjaTdJ+kkDSi4z3ySdKhkv4saY2kFZIuq/Z7OEFlTNIJwPeBL5E8g7Uz8PtMg+q6acA/sg6iCzYnecr/UyTPwR0IfBhqZu2KsuenzLk3gPHAMJJZXkZAutBVbTsbWJt1EF0h6RDgt8B3Sf5/RgBXVv19PEgiO5I2A/4X+FZE/DTreKpB0kHAj4CvALdGRK3PVgKApC+SjA7dq8PGGZO0BDg/Iq5Kv3838Awwqo3HL2qCpMOBX0VE8cP0NUPSLsCtwNHAQ8AOEfFstlFVTtL9wB8iYnJ3vo+voLK1C7A9MFDS3yW9KOm/06HuNUfSlsAVwCnAaxmHU20fofTD37nS1vyUJLOh7JlVXFVSE/8HbUn/IL2a5I+3lmyj6bx0IoP9gPWSFqTde/dIqvp0R05Q3UTSz9M+5ra2C0gmjIRkNvWPAyNJZsa4SVJurjzKPBdIli+5KSIezDLe9lRwLoX7nAV8EKiFe4OVzk9ZEyQdTfKHz5lZx9IFZwLPR8QNWQfSRVuT5I5TSJ4/3R64DbhF0tBqvpG7+LqJpIFAezfV15Lcb3oYOCUirkz3Gwq8AuweEX/r5jDLUua57Etyf2CviFib9lHfkbcuvnLOJSLWFrQ/G5gEHBoRj3V3fF1V8POzT0Q8XFC+EjghImru/qakY0genj86Iu7OOp7OSHtF7gGaIuL5dDmgf1CDXXzpVXoL8O2ImJqWCXgZGBcRt1TrvXL14VFP0iU71rTXJl2eYx1Q6q+E3PzlUOa5HEpyo3Rp8rNKX6CPpBXAiRFxU7cHWoZyzqWVpP8kmerq4IiodCmVTEREi6TW+Skfhg7np8w1SSeSzPzyqYj4U9bxdMEHSRZBfSz9/WjtvXpU0tSIqPoIuO4SESslLaYHPrd8BZUxSZeSjBI7HHiRZFTMh0iuRN7IMrZKpPMgFnYhHQD8mqTb8qWIWJdFXJ0laQbwWeDD6T2cmpE+pvBvwFjgJeAqYFBEjM00sApJOgP4OjA2z93G5UjvzxYOkR8B3A/8P+DvRWvQ5Z6kr5B0WR4GPEUyCvlsYNd04djqvI8TVLYk9ScZZn4csBG4DzgzImp5mDZ57eIrh6QdSRa7fJW3DvZYEhG7l9wpR9TO/JRZxlUpSQG8DmwoLC+cW7NW1XIXH2zq0vsmyYKtA0hGJJ5d2K1clfdxgjIzszzyKD4zM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJygzM8slJyizOiTpYEkh6eMFZaMkvSjpR1nGZlYuL1hoVqck3QUMiIj3SxpCslrzP4AjI+KNbKMz65gTlFmdknQgcC/wMeAcYDvggxGxJtPAzMrkBGVWxyTdDrwfaAHGRMSz2UZkVj7fgzKrb88AWwJfd3KyWuMrKLM6JWkCcAnwBLA+IvbPOCSzijhBmdUhSR8FbgZOBp4C7gc+ERG3ZhqYWQWcoMzqjKTdgT8BP46IqWnZ7cCQiNgv0+DMKuAEZVZHJG0LPAA0A5+N9Bdc0kHAH4BPRsTNGYZoVjYnKDMzyyWP4jMzs1xygjIzs1xygjIzs1xygjIzs1xygjIzs1xygjIzs1xygjIzs1xygjIzs1z6/5r3P3TNhyC0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# vector of x values for plotting\n",
    "x_plot = np.linspace(X_low, X_high, 1000)\n",
    "\n",
    "\n",
    "\n",
    "# simulate process at current t\n",
    "sim_x = simulateOU_GaussianStart(\n",
    "    theta, kappa, sigma,nSim_x_interior)\n",
    "\n",
    "\n",
    "x_plot = tf.cast(tf.reshape(x_plot, shape=[\n",
    "    1000, 1]), tf.float32)\n",
    "\n",
    "u = model(tf.stack([x_plot[:, 0]], axis=1))\n",
    "u = u.numpy().reshape(-1,1)\n",
    "\n",
    "# p = np.exp(-u)\n",
    "p = u\n",
    "x_plot_orig = np.linspace(X_low[0], X_high[0], 1000)\n",
    "# print()\n",
    "\n",
    "# density = p/np.trapz(p.reshape(x_plot_orig.shape), x_plot_orig)\n",
    "density = p\n",
    "\n",
    "\n",
    "# plot histogram of simulated process values and overlay estimated density\n",
    "plt.hist(sim_x, bins=40, density=True, color='b')\n",
    "plt.plot(x_plot, density, 'r', linewidth=2.5)\n",
    "\n",
    "# subplot options\n",
    "plt.ylim(ymin=0.0, ymax=0.45)\n",
    "plt.xlabel(r\"$x$\", fontsize=15, labelpad=10)\n",
    "plt.ylabel(r\"$p(t,x)$\", fontsize=15, labelpad=20)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
